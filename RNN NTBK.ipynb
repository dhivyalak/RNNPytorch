{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <strong>Recurrent Neural Networks Using Pytorch</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook you will learn the basics of a Recurrent Neural Network using the python library Pytorch.\n",
    "---\n",
    "### <strong>Table of Contents</strong>\n",
    "1. [Introduction to Recurrent Neural Networks](#intro)\n",
    "2. [Time Series Data](#time)\n",
    "3. [Using Pytorch](#pytorch)\n",
    "4. [Code](#code)\n",
    "---\n",
    "### By the end of this notebook, you should be able to implement a basic RNN using Pytorch with the provided data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <a name=\"intro\"></a> <strong>Introduction</strong>\n",
    "---\n",
    "*<strong>Recurrent neural networks</strong>*, or RNNs, are widely used in a variety of mediums. RNNs leverage sequential data to make predictions. **Sequential memory** makes it easier for the neural network to recognize patterns and replicate the input. In order to achieve learning through sequential memory, a **feedforward neural network** with looping mechanisms is implemented. \n",
    "\n",
    "As the image below outlines, there are *three* layers: **input, hidden and output**. There are loops that pass previous information forward, allowing the model to *sequentially* store and learn the data. The complexity of a hidden state is based on how much “historic” information is being stored, it is a representation of all previous steps. When training a model, once there is a prediction from a given output, a **loss function** is used to determine the error between the predicted output and real output. The model is trained through back propagation. The weight of each node in the neural network is adjusted with their corresponding gradient that is calculated during **back propagation**. \n",
    "<br>\n",
    "<br>\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/rnnImg.png\">\n",
    "</p>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "The advantage of using sequential data to successfully predict certain outcomes is especially relevant when analyzing **time series data**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <a name=\"time\"></a><strong>Time Series Data</strong>\n",
    "---\n",
    "Prior to training a model, it is important to understand the type of data you are working with. There are many different types of data, this notebook incorporates time series data. In essence, time series data is a collection of chronologically collected observations made over a period of time- sometimes during specific intervals. Time series data can be grouped as either *<strong>metrics</strong>* or *<strong>events</strong>*. \n",
    "\n",
    "* **Metrics**: measurements taken at regular intervals.\n",
    "\n",
    "* **Events**: measurements taken at irregular intervals. \n",
    "\n",
    "Distinguishing if the data is comprised of metrics or events is critical. Events are not condusive for creating predictive models. The irregular intervals between each data point prevents sequential logic from creating patterns on past behavior. In contrast, the characteristic regularity between each metric allows machine learing models to learn from previous data and construct possible outcomes for the future. Creating an RNN using time series data, specifically metrics, is a great way to take advantage of the sequential learning pattern they leverage. \n",
    "\n",
    "Furthermore, time series data can also be categorized as *<strong>linear</strong>* or *<strong>non-linear</strong>*. Based on the mathematical relationship created by the model, the data is classified as one or the other.\n",
    "\n",
    "Popular examples of time series data include weather, stock, and health care data. In this notebook, we will be using stock data to create an RNN model to predict the value of the given stock. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <a name=\"LSTM\"></a><strong>Long-Short Term Memory</strong>\n",
    "---\n",
    "LSTMs, or long short term memory, is a type of RNN used to keep track of long term dependencies. [LSTMs](https://developer.ibm.com/tutorials/iot-deep-learning-anomaly-detection-1/) are necessary when processing tiem series data because they hold memory, unlike traditional RNNs. This feature allows for patterns to be identified and learned by the model. The architecture of long short term memory is dependent on $tanh$ and $sigmoid$ functions implemented in the network. The $tahn$ function ensures that the values in the network remain between -1 and 1 while the $sigmoid$ function regulates if data should be remembered or forgotten. Furthermore, an LSTM has an internal state variable that is modified based on weights and biases through operation gates. Traditionally, an LSTM is comprised of three operation gates: the forget gate, input gate, and output gate. \n",
    "\n",
    "The mathematical representations of each gate are as follows:\n",
    "\n",
    "<strong>Forget Gate</strong>: $$f_t = \\sigma(w_f*[h_{t-1},x_t] + b_f)$$\n",
    "\n",
    "<strong>Input Gate</strong>: $$i_t = \\sigma(w_f*[h_{t-1},x_t] + b_i)$$\n",
    "\n",
    "<strong>Output Gate</strong>: $$O_t = \\sigma(w_f*[h_{t-1},x_t] + b_o)$$\n",
    "\n",
    "Where:  \n",
    "* $w_f$ = weight matrix between forget and input gate\n",
    "* $h_{t-1}$ = previous hidden state\n",
    "* $x_t$ = input\n",
    "* $b_f$ = connection bias at forget gate \n",
    "* $b_i$ = connection bias at input gate \n",
    "* $b_o$ = connection bias at output gate \n",
    "\n",
    "\n",
    "Each gate modifies the input a different way. The forget gate determines what data is relevant to keep and what information can be \"forgotten\". The input gate analyzes what information needs to be added to the current step, and the output gate finalizes the proceeding hidden state. Each of these gates allows for sequential data to be efficiently stored and analyzed, allowing for an accurate predictive model to be developed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <a name=\"pytorch\"></a><strong>Using Pytorch</strong>\n",
    "---\n",
    "Pytorch is a python library that uses the specialized data structure Tensors to encode model parameters and inputs. The following is a brief tutorial on imports that we will be using to evaluate the stock data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use Pytorch, we must first import the library into our workspace. To do this type the following code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to work with data and create a neural network, we can use the Pytorch class nn and the primatives DataLoader and datasets. Dataset is meant to wrap an iterable around the dataset while DataLoader is meant to load and store the desired data. The matplotlib import allows us to change, create and plot a figure in a plotting area. This is useful for the model we are trying to create in this exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what imports to use, we are ready to begin creating our model for our stock data set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <a name=\"code\"></a><strong>Code</strong>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9400, 6)\n",
      "[ 28.75  27.25  25.25 ... 164.94 172.77 168.34]\n"
     ]
    }
   ],
   "source": [
    "stock_data = pd.read_csv(body)\n",
    "stock_data.head()\n",
    "\n",
    "#get how many elements are in data\n",
    "stock_data.shape\n",
    "print(stock_data.shape)\n",
    "stock_data.dropna(subset=['High','Low','Open','Close'], axis = 0, inplace =True)\n",
    "stock_data = stock_data.sort_values(by=\"Date\")\n",
    "#stock_data = stock_data[['High','Low','Open','Close']]\n",
    "stock_data = stock_data['Close']\n",
    "stock_data = stock_data.values.astype(float)\n",
    "print(stock_data)\n",
    "#stock_data = stock_data.drop(columns=['Date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9370,)\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "#use 30 as test number initially\n",
    "test_close_size = 30\n",
    "train_close = stock_data[:-test_close_size]\n",
    "test_close = stock_data[-test_close_size:]\n",
    "print(train_close.shape)\n",
    "print(len(test_close))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9370])\n"
     ]
    }
   ],
   "source": [
    "#train data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#normalize data\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "train_close_normalized = scaler.fit_transform(train_close .reshape(-1, 1))\n",
    "train_close_normalized = torch.FloatTensor(train_close_normalized).view(-1)\n",
    "\n",
    "print(train_close_normalized.shape)\n",
    "\n",
    "\n",
    "train_window = 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length =  9370\n",
      "[(tensor([-0.9486, -0.9530, -0.9588, -0.9570, -0.9548, -0.9501, -0.9461]), tensor([-0.9425])), (tensor([-0.9530, -0.9588, -0.9570, -0.9548, -0.9501, -0.9461, -0.9425]), tensor([-0.9378])), (tensor([-0.9588, -0.9570, -0.9548, -0.9501, -0.9461, -0.9425, -0.9378]), tensor([-0.9291])), (tensor([-0.9570, -0.9548, -0.9501, -0.9461, -0.9425, -0.9378, -0.9291]), tensor([-0.9277]))]\n"
     ]
    }
   ],
   "source": [
    "def create_inout_sequences(input, tw):\n",
    "    inout_seq = []\n",
    "    L = len(input)\n",
    "    print('Length = ',L)\n",
    "    for i in range(L-tw):\n",
    "        train_seq = input[i:i+tw]\n",
    "        train_label = input[i+tw:i+tw+1] \n",
    "        inout_seq.append((train_seq ,train_label))\n",
    "        \n",
    "    return inout_seq\n",
    "\n",
    "train_inout_seq = create_inout_sequences(train_close_normalized, train_window)\n",
    "\n",
    "print(train_inout_seq[:4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_layer_size=100, output_size=1):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "        self.hidden_cell = (torch.zeros(1,1,self.hidden_layer_size),\n",
    "                            torch.zeros(1,1,self.hidden_layer_size))\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        lstm_out, self.hidden_cell = self.lstm(input_seq.view(len(input_seq), 1, -1), self.hidden_cell)\n",
    "        predictions = self.linear(lstm_out.view(len(input_seq), -1))\n",
    "        return predictions[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM()\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "for i in range(epochs):\n",
    "    for seq, labels in train_inout_seq:\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                        torch.zeros(1, 1, model.hidden_layer_size))\n",
    "\n",
    "        y_pred = model(seq)\n",
    "\n",
    "        single_loss = loss_function(y_pred, labels)\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if true:\n",
    "        print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')\n",
    "\n",
    "print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.567732572555542, -0.5760671496391296, -0.5600347518920898, -0.5701345801353455, -0.5823469758033752, -0.5804659128189087, -0.5609607696533203]\n"
     ]
    }
   ],
   "source": [
    "fut_pred = 12\n",
    "\n",
    "test_inputs = train_close_normalized[-train_window:].tolist()\n",
    "print(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "for i in range(fut_pred):\n",
    "    seq = torch.FloatTensor(test_inputs[-train_window:])\n",
    "    with torch.no_grad():\n",
    "        model.hidden = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                        torch.zeros(1, 1, model.hidden_layer_size))\n",
    "        test_inputs.append(model(seq).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[162.58053717]\n",
      " [162.60758022]\n",
      " [162.63070996]\n",
      " [162.64854643]\n",
      " [162.66181051]\n",
      " [162.67167617]\n",
      " [162.6789261 ]\n",
      " [162.68432235]\n",
      " [162.68835925]\n",
      " [162.69126334]\n",
      " [162.69348775]\n",
      " [162.69511486]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "actual_predictions = scaler.inverse_transform(np.array(test_inputs[train_window:] ).reshape(-1, 1))\n",
    "print(actual_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong>Want to Learn More?</strong>\n",
    "\n",
    "Running deep learning programs usually needs a high performance platform. **PowerAI** speeds up deep learning and AI. Built on IBM’s Power Systems, **PowerAI** is a scalable software platform that accelerates deep learning and AI with blazing performance for individual users or enterprises. The **PowerAI** platform supports popular machine learning libraries and dependencies including TensorFlow, Caffe, Torch, and Theano. You can use [PowerAI on IMB Cloud](https://cocl.us/ML0120EN_PAI).\n",
    "\n",
    "Also, you can use **Watson Studio** to run these notebooks faster with bigger datasets. **Watson Studio** is IBM’s leading cloud solution for data scientists, built by data scientists. With Jupyter notebooks, RStudio, Apache Spark and popular libraries pre-packaged in the cloud, **Watson Studio** enables data scientists to collaborate on their projects without having to install anything. Join the fast-growing community of **Watson Studio** users today with a free account at [Watson Studio](https://cocl.us/ML0120EN_DSX). This is the end of this lesson. Thank you for reading this notebook, and good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong>References</strong>\n",
    "\n",
    "* https://pytorch.org/tutorials/beginner/basics/intro.html\n",
    "* https://www.youtube.com/watch?v=LHXXI4-IEns\n",
    "* https://www.influxdata.com/what-is-time-series-data/"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5edc29c2ed010d6458d71a83433b383a96a8cbd3efe8531bc90c4b8a5b8bcec9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}