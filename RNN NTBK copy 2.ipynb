{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <strong>Recurrent Neural Networks Using Pytorch</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook you will learn the basics of a Recurrent Neural Network using the python library Pytorch.\n",
    "---\n",
    "### <strong>Table of Contents</strong>\n",
    "1. [Introduction to Recurrent Neural Networks](#intro)\n",
    "2. [Long-Short Term Memory](#LSTM)\n",
    "3. [Time Series Data](#time)\n",
    "4. [Using Pytorch](#pytorch)\n",
    "5. [Code](#code)\n",
    "---\n",
    "### By the end of this notebook, you should be able to implement a basic RNN using Pytorch with the provided data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <a name=\"intro\"></a> <strong>Introduction</strong>\n",
    "---\n",
    "*<strong>Recurrent neural networks</strong>*, or RNNs, are widely used in a variety of mediums. RNNs leverage sequential data to make predictions. **Sequential memory** makes it easier for the neural network to recognize patterns and replicate the input. In order to achieve learning through sequential memory, a **feedforward neural network** with looping mechanisms is implemented. \n",
    "\n",
    "As the image below outlines, there are *three* layers: **input, hidden and output**. There are loops that pass previous information forward, allowing the model to *sequentially* store and learn the data. The complexity of a hidden state is based on how much “historic” information is being stored, it is a representation of all previous steps. When training a model, once there is a prediction from a given output, a **loss function** is used to determine the error between the predicted output and real output. The model is trained through back propagation. The weight of each node in the neural network is adjusted with their corresponding gradient that is calculated during **back propagation**. \n",
    "<br>\n",
    "<br>\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/rnnImg.png\">\n",
    "</p>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "The advantage of using sequential data to successfully predict certain outcomes is especially relevant when analyzing **time series data**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <a name=\"time\"></a><strong>Time Series Data</strong>\n",
    "---\n",
    "Prior to training a model, it is important to understand the type of data you are working with. There are many different types of data, this notebook incorporates time series data. In essence, time series data is a collection of chronologically collected observations made over a period of time- sometimes during specific intervals. Time series data can be grouped as either *<strong>metrics</strong>* or *<strong>events</strong>*. \n",
    "\n",
    "* **Metrics**: measurements taken at regular intervals.\n",
    "\n",
    "* **Events**: measurements taken at irregular intervals. \n",
    "\n",
    "Distinguishing if the data is comprised of metrics or events is critical. Events are not condusive for creating predictive models. The irregular intervals between each data point prevents sequential logic from creating patterns on past behavior. In contrast, the characteristic regularity between each metric allows machine learing models to learn from previous data and construct possible outcomes for the future. Creating an RNN using time series data, specifically metrics, is a great way to take advantage of the sequential learning pattern they leverage. \n",
    "\n",
    "Furthermore, time series data can also be categorized as *<strong>linear</strong>* or *<strong>non-linear</strong>*. Based on the mathematical relationship created by the model, the data is classified as one or the other.\n",
    "\n",
    "Popular examples of time series data include weather, stock, and health care data. In this notebook, we will be using stock data to create an RNN model to predict the value of the given stock. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <a name=\"LSTM\"></a><strong>Long-Short Term Memory</strong>\n",
    "---\n",
    "LSTMs, or long short term memory, is a type of RNN used to keep track of long term dependencies. [LSTMs](https://developer.ibm.com/tutorials/iot-deep-learning-anomaly-detection-1/) are necessary when processing tiem series data because they hold memory, unlike traditional RNNs. This feature allows for patterns to be identified and learned by the model. The architecture of long short term memory is dependent on $tanh$ and $sigmoid$ functions implemented in the network. The $tahn$ function ensures that the values in the network remain between -1 and 1 while the $sigmoid$ function regulates if data should be remembered or forgotten. Furthermore, an LSTM has an internal state variable that is modified based on weights and biases through operation gates. Traditionally, an LSTM is comprised of three operation gates: the forget gate, input gate, and output gate. \n",
    "\n",
    "The mathematical representations of each gate are as follows:\n",
    "\n",
    "<strong>Forget Gate</strong>: $$f_t = \\sigma(w_f*[h_{t-1},x_t] + b_f)$$\n",
    "\n",
    "<strong>Input Gate</strong>: $$i_t = \\sigma(w_f*[h_{t-1},x_t] + b_i)$$\n",
    "\n",
    "<strong>Output Gate</strong>: $$O_t = \\sigma(w_f*[h_{t-1},x_t] + b_o)$$\n",
    "\n",
    "Where:  \n",
    "* $w_f$ = weight matrix between forget and input gate\n",
    "* $h_{t-1}$ = previous hidden state\n",
    "* $x_t$ = input\n",
    "* $b_f$ = connection bias at forget gate \n",
    "* $b_i$ = connection bias at input gate \n",
    "* $b_o$ = connection bias at output gate \n",
    "\n",
    "\n",
    "Each gate modifies the input a different way. The forget gate determines what data is relevant to keep and what information can be \"forgotten\". The input gate analyzes what information needs to be added to the current step, and the output gate finalizes the next hidden state. Each of these gates allows for sequential data to be chronologically stored and analyzed, allowing for a comprehensive model to be developed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <a name=\"pytorch\"></a><strong>Using Pytorch</strong>\n",
    "---\n",
    "Pytorch is a python library that uses the specialized data structure Tensors to encode model parameters and inputs. The following is a brief tutorial on imports that we will be using to evaluate the stock data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use Pytorch, we must first import the library into our workspace. To do this type the following code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to work with data and create a neural network, we can use the Pytorch class nn and the primatives DataLoader and datasets. Dataset is meant to wrap an iterable around the dataset while DataLoader is meant to load and store the desired data. The matplotlib import allows us to change, create and plot a figure in a plotting area. This is useful for the model we are trying to create in this exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what imports to use, we are ready to begin creating our model for our stock data set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <a name=\"code\"></a><strong>Code</strong>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "!pip install pandas\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following empty code cell, import the Stock Data CSV file to your notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert stock data csv import here, follow the directions outlined in the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get how many elements are in data\n",
    "stock_data = pd.read_csv(#name of csv file import here)\n",
    "print(stock_data.head(5))\n",
    "print(stock_data.tail(5))\n",
    "print(stock_data.shape)\n",
    "print(stock_data.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "price = stock_data[['High','Low','Open','Close']]\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "price = scaler.fit_transform(price.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_window = 7 \n",
    "import numpy as np\n",
    "def create_inout_sequences(input, tw):\n",
    "    inout_seq = []\n",
    "    L = len(input)\n",
    "    print('Length = ',L)\n",
    "    for i in range(L-tw):\n",
    "        #inout_seq.append(input[i: i + tw])\n",
    "        data_seq = input[i:i+tw]\n",
    "        #train_seq = torch.Tensor(train_seq)\n",
    "        data_label = input[i+tw:i+tw+1][0][3]\n",
    "        #train_label = torch.Tensor(np.array(train_label))\n",
    "        inout_seq.append((data_seq ,data_label))\n",
    "    \n",
    "    #data = np.array(inout_seq);\n",
    "    data = inout_seq;\n",
    "    print('size of data : ', len(data))\n",
    "    #test_set_size = int(np.round(0.2*len(data)));\n",
    "    test_set_size = 20\n",
    "    train_set_size = len(data) - (test_set_size);\n",
    "    print('size of test : ', test_set_size)\n",
    "    print('size of train : ', train_set_size)\n",
    "    \n",
    "    #data_train = data[]\n",
    "    #x_train = data[:train_set_size,:-1]\n",
    "    #y_train = data[:train_set_size,-1]\n",
    "    train = data[:train_set_size]\n",
    "    test = data[train_set_size:]\n",
    "    \n",
    "    #print(x_train)\n",
    "    #print(y_train)\n",
    "    \n",
    "    '''x_test= data[train_set_size:,0]\n",
    "    y_train = data[train_set_size:,1]\n",
    "    #x_test = data[train_set_size:,:-1]\n",
    "    #y_test = data[train_set_size:,-1]\n",
    "    \n",
    "    x_train = torch.from_numpy(x_train).type(torch.Tensor)\n",
    "    x_test = torch.from_numpy(x_test).type(torch.Tensor)[0][0]\n",
    "    y_train_lstm = torch.from_numpy(y_train).type(torch.Tensor)\n",
    "    y_test_lstm = torch.from_numpy(y_test).type(torch.Tensor)[0][0]\n",
    "    \n",
    "    return [x_train, y_train_lstm, x_test, y_test_lstm]'''\n",
    "    return train,test\n",
    "    \n",
    "\n",
    "train,test = create_inout_sequences(price, train_window )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=4, hidden_layer_size=100, output_size=1):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "        self.hidden_cell = (torch.zeros(1,1,self.hidden_layer_size),\n",
    "                            torch.zeros(1,1,self.hidden_layer_size))\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        #print('reached')\n",
    "        lstm_out, self.hidden_cell = self.lstm(input_seq.view(len(input_seq), 1, -1), self.hidden_cell)\n",
    "        predictions = self.linear(lstm_out.view(len(input_seq), -1))\n",
    "        return predictions[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM()\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "for i in range(epochs):\n",
    "    for seq, labels in train:\n",
    "        seq = torch.from_numpy(np.array(seq)).type(torch.Tensor)\n",
    "        #print(seq)\n",
    "        labels = torch.from_numpy(np.array(labels)).type(torch.Tensor)\n",
    "        #print(labels)\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                        torch.zeros(1, 1, model.hidden_layer_size))\n",
    "\n",
    "        y_pred = model(seq)\n",
    "        #print('y_pred : ',y_pred)\n",
    "        labels = labels.view(1)\n",
    "        #print('label : ', labels)\n",
    "        single_loss = loss_function(y_pred, labels)\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "actual=[]\n",
    "pred = []\n",
    "#for i in range(fut_pred):\n",
    "if True: \n",
    "    #seq = test_inout_seq\n",
    "    for seq, labels in test:\n",
    "        seq = torch.from_numpy(np.array(seq)).type(torch.Tensor)\n",
    "        #print(seq)\n",
    "        labels = torch.from_numpy(np.array(labels)).type(torch.Tensor)\n",
    "        actual.append(labels)\n",
    "        with torch.no_grad():\n",
    "            model.hidden = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                        torch.zeros(1, 1, model.hidden_layer_size))\n",
    "            pred.append(model(seq).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = torch.from_numpy(np.array(pred)).type(torch.Tensor)\n",
    "actual = torch.from_numpy(np.array(actual)).type(torch.Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred)\n",
    "print(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pred_new = scaler.inverse_transform( np.c_[ np.zeros(20),np.zeros(20),np.zeros(20),np.array(pred)])\n",
    "print(pred_new[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_new = scaler.inverse_transform( np.c_[ np.zeros(20),np.zeros(20),np.zeros(20),np.array(actual)])\n",
    "print(actual_new[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#fig = plt.figure()\n",
    "#ax1 = fig.add_subplot(1, 2, 1)\n",
    "#ax2 = fig.add_subplot(1, 2, 2, sharey=ax1)\n",
    "\n",
    "#predicted value in red\n",
    "plt.plot(pred_new[:,3], 'r+')\n",
    "#actual value in green\n",
    "plt.plot(actual_new[:,3], 'go')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 1) # row 1, col 2 index 1\n",
    "plt.plot(pred_new)\n",
    "plt.title(\"predicted chart\")\n",
    "plt.xlabel('X-axis ')\n",
    "plt.ylabel('Y-axis ')\n",
    "\n",
    "plt.subplot(1, 2, 2) # index 2\n",
    "plt.plot(actual_new)\n",
    "plt.title(\"actual chart\")\n",
    "plt.xlabel('X-axis ')\n",
    "plt.ylabel('Y-axis ')\n",
    "\n",
    "#sharex=ax1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4), dpi=80, facecolor='w', edgecolor='k')\n",
    "#plt.plot(stock_data,color=\"r\",label=\"true-result\")\n",
    "plt.plot(pred_new,color=\"g\",label=\"predicted-result\")\n",
    "plt.legend()\n",
    "plt.title(\"Stock Close Prediction\")\n",
    "plt.xlabel(\"Days from 1980-2018\")\n",
    "plt.ylabel(\"Close Values\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    fig = plt.figure()\n",
    "    a1 = fig.add_axes([0,0,1,1])\n",
    "    #x = common\n",
    "    a1.plot(actual_new, 'ro')\n",
    "    a1.set_ylabel('Actual')\n",
    "    a2 = a1.twinx()\n",
    "    a2.plot( pred_new,'o')\n",
    "    a2.set_ylabel('Predicted')\n",
    "    fig.legend(labels = ('Actual','Predicted'),loc='upper left')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong>Want to Learn More?</strong>\n",
    "\n",
    "Running deep learning programs usually needs a high performance platform. **PowerAI** speeds up deep learning and AI. Built on IBM’s Power Systems, **PowerAI** is a scalable software platform that accelerates deep learning and AI with blazing performance for individual users or enterprises. The **PowerAI** platform supports popular machine learning libraries and dependencies including TensorFlow, Caffe, Torch, and Theano. You can use [PowerAI on IMB Cloud](https://cocl.us/ML0120EN_PAI).\n",
    "\n",
    "Also, you can use **Watson Studio** to run these notebooks faster with bigger datasets. **Watson Studio** is IBM’s leading cloud solution for data scientists, built by data scientists. With Jupyter notebooks, RStudio, Apache Spark and popular libraries pre-packaged in the cloud, **Watson Studio** enables data scientists to collaborate on their projects without having to install anything. Join the fast-growing community of **Watson Studio** users today with a free account at [Watson Studio](https://cocl.us/ML0120EN_DSX). This is the end of this lesson. Thank you for reading this notebook, and good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong>References</strong>\n",
    "\n",
    "* https://pytorch.org/tutorials/beginner/basics/intro.html\n",
    "* https://www.youtube.com/watch?v=LHXXI4-IEns\n",
    "* https://www.influxdata.com/what-is-time-series-data/"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5edc29c2ed010d6458d71a83433b383a96a8cbd3efe8531bc90c4b8a5b8bcec9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}